mode: train
data:
  train_paths:
    - /data/sls/r/u/hengjui/home/scratch/dataset/miniasr_data/timit_phone/train/data_list_sorted.json
  dev_paths:
    - /data/sls/r/u/hengjui/home/scratch/dataset/miniasr_data/timit_phone/dev/data_list_sorted.json
  text:
    mode: phone
    vocab: /data/sls/r/u/hengjui/home/scratch/dataset/miniasr_data/timit_phone/train/vocab_word.txt

model:
  name: ctc_asr
  extractor:
    name: fbank
    train: false
  cnn:
    hid_dim: 64
    out_dim: 256
  # cif:
  #   hid_dim: 256
  #   threshold: 1.0
  #   downsample: 8.0
  #   calc_weight: sim-l2
  #   window_size: 5
  # cnngt:
  #   hid_dim: 32
  #   out_dim: 256
  #   downsample: 4.0
  encoder:
    module: conformer
    d_model: 256
    num_layers: 4
    attention_module: pytorch  # vq
    num_heads: 4
    ffn_size: 1024
    dropout: 0.1
    eps: 1.e-6
    norm_first: false
    max_len: 3600
    # codebook_size: 32
  optim:
    algo: Adam
    kwargs:
      lr: 0.0005
      weight_decay: 1.e-6
    scheduler:
      warmup_step: 4000
      decay_type: poly
  specaugment:
    freq_mask_range: [0, 25]
    freq_mask_num: 2
    time_mask_range: [0, 70]
    time_mask_num: 2
    time_mask_max: 1.0
    time_warp_w: 80

hparam:
  train_batch_size: 32
  val_batch_size: 32
  accum_grad: 1
  grad_clip: 5
  njobs: 16
  pin_memory: true

checkpoint_callbacks:
  monitor: val_wer
  mode: min
  save_top_k: 1  # -1: all ckpts will be saved

trainer:
  max_epochs: 500
  max_steps: 200000
  check_val_every_n_epoch: 5
  gpus: 1
  precision: 16
  logger: wandb
  default_root_dir: model/ctc_conf-cnngt_timit_phone_1
  deterministic: false

wandb:
  project: VQSA
